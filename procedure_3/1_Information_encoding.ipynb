{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28773843",
   "metadata": {},
   "source": [
    "# Biocomputing with Brainoware\n",
    "### Procedure 3 - Brainoware software framework\n",
    "#### 1. Information encoding\n",
    "**Author: Huiyu Chu**  \n",
    "**Date: June 6, 2025**  \n",
    "**Description**: This part describes how to convert audio clips (or already processed audio features, like cepstral coefficients) to binary spatiotemporal pulses that will be delivered to stimulation electrodes in the next step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d952e4",
   "metadata": {},
   "source": [
    "1. (Optional) Convert the raw speech or audio dataset to matrices of cepstral coefficients. Python audio libraries like librosa and spafe provide a straightforward interface to achieve this. Take librosa for example, load each audio clip in the dataset and calculate the Mel-Frequency Cepstral Coefficients  (MFCC), and save the converted coefficient matrices for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b0cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of MFCC features: (2, 44, 12)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assume one has a audio dataset in the \"1_Speech_Commands_dataset\" subfolder of current working folder which only contains all audio files. For display purposes, we use only two single-word speech files from Speech Commands dataset to demonstrate how raw audio files can be converted to a cepstral feature matrix. \n",
    "## Speech Commands Dataset credits: Warden, Pete. \"Speech commands: A public dataset for single-word speech recognition.\" Dataset available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz (2017)\n",
    "\n",
    "dataset_path = \"./1_Speech_Commands_dataset/\"\n",
    "audio_files = [\n",
    "    dataset_path + filename \n",
    "    for filename in os.listdir(dataset_path)\n",
    "    ]\n",
    "\n",
    "def compute_mfcc(audio_path, n_mfcc=12):\n",
    "    \"\"\"\n",
    "    function to calculate MFCC coefficients of an audio file\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): path to the audio file\n",
    "        n_mfcc (int, optional): number of coefficients to return\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray [shape=(order, t)]: n_mfcc coefficients of all time windows\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfccs\n",
    "\n",
    "# Compute mfcc features of each audio file\n",
    "mfcc_features = np.array(\n",
    "    [compute_mfcc(audio_file).T for audio_file in audio_files]\n",
    "    )\n",
    "print(\"Shape of MFCC features:\", mfcc_features.shape)\n",
    "\n",
    "# Save processed dataset in .npy format with the shape (num_audios, num_time_windows, num_mfcc) \n",
    "save_path = dataset_path + \"processed_dataset/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "np.save(f\"{save_path}/mfcc_features.npy\", mfcc_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02fefe",
   "metadata": {},
   "source": [
    "2. The previous step can be skipped, because the original Japanese Vowels Dataset used in the paper [Cai et al.,2023] has already been converted to cepstral coefficients through Linear Predictive Coding (LPC) - another common speech feature extraction method. However, it requires additional preprocessing. Download the Japanese Vowels Dataset using this link (https://archive.ics.uci.edu/static/public/128/japanese+vowels.zip), unzip it to the /1_Japanese_Vowels_dataset subfolder in the current working directory, and generate the feature matrix with the same shape structure as in Step 72:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bb48bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LPCC features: (240, 29, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load the Japanese Vowels training dataset in ./1_Japanese_Vowels_dataset/ae.train\n",
    "with open('./1_Japanese_Vowels_dataset/ae.train', 'r') as f:\n",
    "    all_content = f.read()\n",
    "\n",
    "# Separate LPCC-represented speech clips with 2 line breaks\n",
    "speech_blocks = all_content.strip().split('\\n\\n')\n",
    "\n",
    "# Convert all blocks into a NumPy array\n",
    "lpcc_features = []\n",
    "for block in speech_blocks:\n",
    "    lines = block.strip().split('\\n')\n",
    "    lines_to_append = 29-len(lines) # append \"nan\" lines to make each block have 29 lines\n",
    "    for _ in range(lines_to_append):\n",
    "        lines.append(\" \".join([\"nan \"]*12))\n",
    "    matrix = [list(map(lambda x: float(x) if x.lower() != 'nan' else np.nan, line.strip().split())) for line in lines]\n",
    "    lpcc_features.append(matrix)\n",
    "lpcc_features_np = np.array(lpcc_features)[:240]  # Shape: (240, 29, 12)\n",
    "\n",
    "print(\"Shape of LPCC features:\", lpcc_features_np.shape)\n",
    "save_path = \"./1_Japanese_Vowels_dataset/processed_dataset\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "np.save(f\"{save_path}/lpcc_features.npy\", lpcc_features_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07287f48",
   "metadata": {},
   "source": [
    "3. Now, the raw audio dataset has been converted to a three-dimensional NumPy cepstral coefficients matrices (e.g. Japanese Vowel Dataset has a dimension of (240, 29, 12)) where the first dimension is the number of audio clips, the second dimension is the number of time windows in each audio clip, and the third dimension is the number of cepstral coefficients at each time window. Now, normalize the matrices to the range of [0,1] and perform binary thresholding to convert float coefficients to {0,1} binaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7102d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpcc_features_nanmax = np.nanmax(\n",
    "    lpcc_features_np, \n",
    "    axis=(1, 2), \n",
    "    keepdims=True)\n",
    "lpcc_features_nanmin = np.nanmin(\n",
    "    lpcc_features_np, \n",
    "    axis=(1, 2), \n",
    "    keepdims=True)\n",
    "lpcc_features_norm = (\n",
    "    lpcc_features_np - lpcc_features_nanmin) / (lpcc_features_nanmax - lpcc_features_nanmin)\n",
    "lpcc_features_norm = np.nan_to_num(lpcc_features_norm)\n",
    "lpcc_features_binary = np.where(lpcc_features_norm > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d9a03",
   "metadata": {},
   "source": [
    "4. For each time window in each audio clip, collect non-zero indices, which will act as indices of stimulation electrodes during the stimulation stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6a01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "selected_stimulation_electrode_indices = []\n",
    "for i in range(lpcc_features_binary.shape[0]):\n",
    "    selected_stimulation_electrode_indices_per_clip = []\n",
    "    for j in range(lpcc_features_binary.shape[1]):\n",
    "        selected_stimulation_electrode_indices_per_clip.append(list(np.nonzero(lpcc_features_binary[i][j])[0]))\n",
    "    selected_stimulation_electrode_indices.append(selected_stimulation_electrode_indices_per_clip)\n",
    "\n",
    "save_path = \"./1_Japanese_Vowels_dataset/processed_dataset\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "with open(f'{save_path}/selected_stimulation_electrode_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(selected_stimulation_electrode_indices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e68c8",
   "metadata": {},
   "source": [
    "[PAUSE POINT] This concludes the preprocessing of the Japanese Vowel Dataset. The saved two-dimensional list selected_stimulation_electrode_indices determines which stimulation electrodes will be activated at each time step of each audio clip. For example, when the index at time step i is [0, 1, 4], the stimulation electrode #0, #1, and #4 will be activated and go through a bipolar voltage stimulation at time step I, while the other 9 electrodes will remain inactivated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50e3fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
